SPDX-License-Identifier: MIT
path: "/tutorials/install-llama-gpu"
slug: "install-llama-gpu"
date: "2025-02-09"
title: "How to Install and Run Meta Llama Instruct on a GPU Server"
short_description: "Step-by-step guide to installing and running Meta Llama Instruct on a GPU server for optimized AI model execution."
tags: ["AI", "Llama", "Meta AI", "GPU", "Machine Learning", "Linux"]
author: "Stephen Ndegwa"
author_link: "https://github.com/stephenndegwa"
author_img: "https://avatars.githubusercontent.com/u/105418748"
author_description: "System administrator with expertise in Linux, AI model deployment, and high-performance computing."
language: "en"
available_languages: ["en"]
header_img: "header-ai"
cta: "product"

## Introduction

Meta Llama Instruct is a powerful large language model (LLM) designed for instruction-following tasks. Running Llama Instruct requires GPU acceleration for optimal performance due to its computational demands. This guide will walk you through the process of setting up and running Meta Llama Instruct on a GPU server running Ubuntu.

We will cover:

- Connecting to the server
- Installing necessary drivers and dependencies
- Logging into Hugging Face and requesting model access
- Running the Llama Instruct model
- Deploying an API for remote access
- Troubleshooting common issues
- Optimizing performance for large-scale inference

---

## Prerequisites

Before proceeding, ensure you have:

- A dedicated GPU server with an NVIDIA GPU (e.g., RTX 3090, A100, GTX 1080 or better)
- Ubuntu (20.04 or 22.04) installed
- SSH access to the server
- At least 50GB of free disk space for model downloads and execution
- Basic knowledge of Linux command line
- A [Hugging Face account](https://huggingface.co/) for model access
- Request access to the Meta Llama model via Hugging Face by following these steps:

### How to Request Access to Meta Llama on Hugging Face

1. Visit the [Meta Llama model page](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) on Hugging Face.
2. Click the 'Request Access'.
3. Log in to your Hugging Face account (or create one if you donâ€™t have an account).
4. Fill out the request form, agreeing to the terms and conditions.
5. Wait for an email confirmation that your access has been granted.
6. Once access is granted, authenticate using the Hugging Face CLI.

```bash
huggingface-cli login
```

This step ensures you have permission to download and run the model.

---

## Step 1: Connect to Your GPU Server

### Using SSH to Access the Server

1. Open a terminal on your local machine.
2. Connect to your server via SSH:
   ```bash
   ssh user@your-server-ip
   ```
3. Update and upgrade system packages:
   ```bash
   sudo apt update && sudo apt upgrade -y
   ```

If you encounter connection issues, check firewall settings or contact your provider.

---

## Step 2: Install NVIDIA Drivers and CUDA

### Verify Available GPUs

Before proceeding, check your GPU status:

```bash
nvidia-smi
```

You should see a table displaying GPU information, including driver versions and memory usage, if not please proceed and install nvidia drivers.

### Install NVIDIA Drivers

```bash
sudo ubuntu-drivers install
```

### Install CUDA Toolkit (Latest Supported Version)

Download and install CUDA:

```bash
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda-repo-ubuntu2204-12-4-local_12.4.0-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-4-local_12.4.0-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2204-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-4
```

### Verify Installation

Restart the system and check if CUDA is installed:

```bash
sudo reboot
nvidia-smi
```

---

## Step 3: Install Dependencies and Authenticate with Hugging Face

### Install Python and Pip

```bash
sudo apt install -y python3 python3-pip
```

### Install PyTorch with CUDA Support

```bash
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

### Install Other Required Libraries

```bash
pip3 install transformers sentencepiece accelerate flask
```

---


## Step 4: Run Meta Llama Instruct Model

### Load and Interact with the Model Using Transformers

To create and edit a script, use `nano`:

```bash
nano run_llama.py
```

Then paste the following:

```python
import torch
from transformers import pipeline

# Define the model ID; you can replace this with a different Llama model
model_id = "meta-llama/Llama-3.2-1B-Instruct"

# Load the text-generation pipeline with the specified model, ensuring it runs on GPU
pipe = pipeline(
    "text-generation",  # Define the task
    model=model_id,  # Specify the model to use
    torch_dtype=torch.bfloat16,  # Use bfloat16 for efficiency
    device_map="auto",  # Automatically select the best available device (GPU preferred)
)

def generate_response(prompt):
    """Generate a response from the model given a text prompt."""
    outputs = pipe(prompt, max_new_tokens=256)  # Generate up to 256 new tokens
    return outputs[0]["generated_text"]  # Return the generated text

if __name__ == "__main__":
    prompt = input("Enter your prompt: ")  # Get input from the user
    response = generate_response(prompt)  # Generate a response using the model
    print("Response:", response)  # Print the generated response
```

Save and exit (`CTRL+X`, then `Y`, then `Enter`).

### Run the Script

```bash
python3 run_llama.py
```

---

## Step 5: Deploy an API

To make this model accessible via a REST API, create an `api.py` file:

```bash
nano api.py
```

Paste the following:

```python
from flask import Flask, request, jsonify
from transformers import pipeline
import torch

app = Flask(__name__)

# Define the model ID; change this if using a different Llama model
model_id = "meta-llama/Llama-3.2-1B-Instruct"

# Load the text-generation pipeline with GPU acceleration
pipe = pipeline(
    "text-generation",  # Define the task
    model=model_id,  # Specify the model
    torch_dtype=torch.bfloat16,  # Use bfloat16 for efficiency
    device_map="auto",  # Automatically select the best available device
)

@app.route("/generate", methods=["POST"])
def generate():
    """API endpoint to generate text based on a given prompt."""
    data = request.json  # Get JSON input from the request
    prompt = data.get("prompt", "Tell me about artificial intelligence.")  # Extract prompt
    response = pipe(prompt, max_new_tokens=256)[0]["generated_text"]  # Generate text
    return jsonify({"response": response})  # Return response as JSON

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)  # Start the API server
```

Save and exit (`CTRL+X`, then `Y`, then `Enter`).

### Start the API

```bash
python3 api.py
```

### Example API Request

```bash
curl -X POST "http://localhost:5000/generate" -H "Content-Type: application/json" -d '{"prompt": "Explain deep learning"}'
```

### Expected Response

```json
{
    "response": "Deep learning is a subset of machine learning that uses neural networks with multiple layers to model complex patterns in data."
}
```

---

## Conclusion

This guide covers the installation and execution of Meta Llama Instruct on a GPU server, including driver setup, authentication, model execution, and API deployment. Following these steps ensures you can effectively use Meta Llama for your AI-powered applications while leveraging GPU acceleration for optimal performance.



### License: MIT

<!--

Contributor's Certificate of Origin

By making a contribution to this project, I certify that:

(a) The contribution was created in whole or in part by me and I have

    the right to submit it under the license indicated in the file; or

(b) The contribution is based upon previous work that, to the best of my

    knowledge, is covered under an appropriate license and I have the

    right under that license to submit that work with modifications,

    whether created in whole or in part by me, under the same license

    (unless I am permitted to submit under a different license), as

    indicated in the file; or

(c) The contribution was provided directly to me by some other person

    who certified (a), (b) or (c) and I have not modified it.

(d) I understand and agree that this project and the contribution are

    public and that a record of the contribution (including all personal

    information I submit with it, including my sign-off) is maintained

    indefinitely and may be redistributed consistent with this project

    or the license(s) involved.

Signed-off-by: Stephen Ndegwa - sndegwa@hostraha.com

-->