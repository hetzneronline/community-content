SPDX-License-Identifier: MIT
path: "/tutorials/install-deepseek-gpu"
slug: "install-deepseek-gpu"
date: "2025-02-06"
title: "How to Install and Run DeepSeek AI Model on a GPU Server"
short_description: "Step-by-step guide to installing and running DeepSeek AI on a GPU server for efficient AI model execution."
tags: ["AI", "DeepSeek", "GPU", "Machine Learning", "Linux"]
author: "Stephen Ndegwa"
author_link: "https://github.com/stephenndegwa"
author_img: "https://avatars.githubusercontent.com/u/105418748"
author_description: "System administrator with expertise in Linux, AI deployment, and high-performance computing."
language: "en"
available_languages: ["en"]
header_img: "header-ai"
cta: "product"

## Introduction

DeepSeek AI is a powerful large language model (LLM) that requires GPU acceleration for optimal performance due to its high computational demands. GPUs are designed for parallel processing, making them significantly faster than CPUs for handling the large-scale matrix operations necessary for deep learning models. Many cloud providers and dedicated server providers offer GPU-accelerated machines that provide the necessary computational power to run DeepSeek AI efficiently. This guide will walk you through the process of setting up and running DeepSeek AI on a Hetzner GPU dedicated server running Ubuntu.

## Prerequisites

Before proceeding, ensure you have:

- A Hetzner GPU dedicated server with an NVIDIA GPU (e.g., GTX 1080 or better)
- Ubuntu (24.04 or 22.04) installed
- SSH access to the server
- At least 20GB of free disk space for model downloads and execution
- Basic knowledge of Linux command line
- A [Hugging Face account](https://huggingface.co/) for model access

---

## Step 1: Connect to Your GPU Server

### Using SSH to Access the Server

1. Open a terminal on your local machine.
2. Connect to your GPU server via SSH:
   ```bash
   ssh user@your-server-ip
   ```
3. Update and upgrade system packages to ensure you have the latest updates:
   ```bash
   sudo apt update && sudo apt upgrade -y
   ```

If you encounter issues connecting, check your firewall settings or contact your server provider's support team.

---

## Step 2: Install NVIDIA Drivers and CUDA

### Verify Available GPUs

Before proceeding, check your GPU status:

```bash
nvidia-smi
```

You should see a table displaying GPU information, including driver versions and memory usage. If not, please go ahead and install NVIDIA drivers as shown below.

### Install NVIDIA Drivers

```bash
sudo ubuntu-drivers install
```

### Install CUDA Toolkit (CUDA 12.4 for compatibility)

Download and install CUDA:

```bash
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda-repo-ubuntu2204-12-4-local_12.4.0-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-4-local_12.4.0-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2204-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-4
```

### Verify Installation

Restart the system and check if CUDA is installed:

```bash
sudo reboot
nvidia-smi
```

---

## Step 3: Install Dependencies

### Install Python and Pip

```bash
sudo apt install -y python3 python3-pip
```

### Install PyTorch with CUDA Support

```bash
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

### Install Other Required Libraries

```bash
pip3 install transformers sentencepiece accelerate flask
```

---

## Step 4: Run DeepSeek AI Model

### Load the Model Using Transformers

To create and edit a script, use `nano`:

```bash
nano run_deepseek.py
```

Then paste the following:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Function to generate a response using DeepSeek AI
# Replace 'model_name' with your preferred DeepSeek AI model

def generate_response(prompt: str):
    model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).cuda()
    
    # Tokenize the input and send it to GPU
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    output = model.generate(**inputs, max_length=500)
    
    # Decode and return the generated response
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Main script execution
if __name__ == "__main__":
    prompt = input("Enter your query: ")  # Get user input
    response = generate_response(prompt)  # Generate response
    print("Response:", response)  # Display the response
```

Save and exit (`CTRL+X`, then `Y`, then `Enter`).

### Run the Script

```bash
python3 run_deepseek.py
```

---

## Step 5: Deploy as an API

To create a REST API, use Flask. Create a script:

```bash
nano api.py
```

Paste the following:

```python
from flask import Flask, request, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

app = Flask(__name__)

# Define the model to be used in the API
# Replace 'model_name' with your preferred DeepSeek AI model
model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).cuda()

@app.route("/generate", methods=["POST"])
def generate():
    data = request.json
    prompt = data.get("prompt", "Tell me about artificial intelligence.")
    
    # Tokenize the input and send it to GPU
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    output = model.generate(**inputs, max_length=500)
    
    # Decode and return the response
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return jsonify({"response": response})

# Run the API
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
```

Save and exit (`CTRL+X`, then `Y`, then `Enter`).

### Start the API

```bash
python3 api.py
```

### Example Usage

Send a request using `curl`:

```bash
curl -X POST "http://localhost:5000/generate" -H "Content-Type: application/json" -d '{"prompt": "Explain deep learning"}'
```

Expected JSON response:

```json
{
    "response": "Deep learning is a subset of machine learning that uses neural networks with multiple layers to model complex patterns in data."
}
```

---

## Conclusion

You have successfully installed and run the DeepSeek AI model on a GPU server running Ubuntu. This guide provides step-by-step instructions for installation, running the model, and deploying a Flask API. You can now integrate the AI model into your applications or services.





### License: MIT




<!--

Contributor's Certificate of Origin

By making a contribution to this project, I certify that:

(a) The contribution was created in whole or in part by me and I have

    the right to submit it under the license indicated in the file; or

(b) The contribution is based upon previous work that, to the best of my

    knowledge, is covered under an appropriate license and I have the

    right under that license to submit that work with modifications,

    whether created in whole or in part by me, under the same license

    (unless I am permitted to submit under a different license), as

    indicated in the file; or

(c) The contribution was provided directly to me by some other person

    who certified (a), (b) or (c) and I have not modified it.

(d) I understand and agree that this project and the contribution are

    public and that a record of the contribution (including all personal

    information I submit with it, including my sign-off) is maintained

    indefinitely and may be redistributed consistent with this project

    or the license(s) involved.

Signed-off-by: Stephen Ndegwa - sndegwa@hostraha.com

-->