---
SPDX-License-Identifier: MIT
path: "/tutorials/ai-crawler-rate-limiting"
slug: "ai-crawler-rate-limiting"
date: "2025-11-01"
title: "AI Crawler Rate Limiting"
short_description: "This tutorial covers how to implement rate limiting for AI crawlers to prevent overloading servers and ensure fair usage."
tags: ["AI", "Crawler", "NGINX", "Rate Limiting", "Tutorial", "LLMSTXT"]
author: "Riad Zejnilagic Trumic"
author_link: "https://github.com/Riiiad"
author_img: "https://avatars.githubusercontent.com/u/20195182?s=96&v=4"
author_description: "From Graphic Design to Development and DevOps, I love building efficient and creative solutions on the web."
language: "en"
available_languages: []
header_img: "header-x"
cta: "tutorial"
---

# AI Crawlers Rate Limiting with nginx and More: Practical Tips to Protect Your Website

Working with different projects and clients, I noticed that protecting your website from unwanted AI crawlers is becoming increasingly important. So today I wanted to share a few practical tips that really helped improve how I manage server resources and prevent unnecessary crawling.

## In The AI Crawler Era: Time to Step Up Your Server Game

Here's the realityâ€”AI crawlers are not going away. OpenAI, Anthropic, Perplexity, Apple, Meta, and countless others are all crawling the web to train their models. And they're crawling *aggressively*. Your server infrastructure needs to adapt.

**Why This Matters Now:**

Unlike traditional search engine bots that crawl occasionally, AI crawlers are relentless. They hit high-traffic sites constantly, consuming bandwidth and CPU cycles. If you're not prepared, your bills go up, your performance tanks, and your legitimate users suffer. The old approach of "just let everyone in" doesn't cut it anymore.

The future of the web is going to be shaped by whoever controls their server resources best. Make sure that's you.
Now lets see some examples and tips to help you get started.

## TIP 1: nginx is Your Best Friend for Rate Limiting

So when it comes to controlling AI crawlers, **nginx** is hands down the most efficient approach. Why? Because nginx applies rate limiting at the server level before requests even touch your application. No wasted resources, no database queries, no unnecessary app logic running.

Here's how to rate limit AI crawlers in your nginx config:
> **Note:** This is simplified example code you need more customization for production use

```nginx
# Detect crawlers
map $http_user_agent $is_crawler {
    default 0;
    ~*(bot|crawl|spider|gpt|claude|anthropic|perplexity|applebot) 1;
}

# Choose the appropriate limit zone name dynamically
map $is_crawler $limit_zone {
    1 crawler_limit;
    0 normal_limit;
}

# Define the rate limit zones
limit_req_zone $binary_remote_addr zone=crawler_limit:10m rate=30r/m;
limit_req_zone $binary_remote_addr zone=normal_limit:10m rate=100r/m;

server {
    listen 80;
    server_name example.com;

    # Apply rate limiting to all requests
    location / {
        limit_req zone=$limit_zone burst=10 nodelay;
    }

    # Other server configurations...
}

```

This approach identifies crawlers by keywords and applies a strict rate limit of 30 requests per minute with a burst allowance of 5. Regular users get a much higher limit of 100 requests per second. Crawlers are allowed to access your site, but at a controlled pace that won't hammer your infrastructure. It's clean, efficient, and fair. Compare this to blocking entirelyâ€”this way you don't prevent legitimate crawlers from accessing your content, you just keep them from overwhelming your server.

If you want to learn more about nginx rate limiting and zones, check out the official nginx documentation or the links at the end of this article.

**Pro Tip: Verify Real Crawlers**

Here's something sneakyâ€”some bots pretend to be OpenAI or Claude crawlers when they're actually something else entirely. To verify if a request is really from OpenAI or Anthropic, check their official IP ranges:

- **OpenAI GPTBot**: https://openai.com/gptbot.json
- **Anthropic Claude**: Check their official documentation for IP ranges

You can validate the IP address against these official lists before applying rate limits. This prevents bad actors from spoofing legitimate crawler user agents. It's an extra layer of protection that separates the real deal from the imposters.

```bash
# Example: Check if an IP is in OpenAI's official range
curl https://openai.com/gptbot.json | grep -i "your_ip"
```

This way, you can be more lenient with verified crawlers and stricter with unknown ones.


## TIP 2: .htaccess Works, But It's Not Optimal

A lot of us still rely on .htaccess for everything. But here's the thingâ€”while it works for blocking crawlers, Apache still processes the request through its pipeline. It's better than letting it reach your app, but it's not as efficient as nginx.

If you're stuck on Apache, here's how to do it:

```apache
SetEnvIf User-Agent "GPTBot" bad_bot
SetEnvIf User-Agent "anthropic-ai" bad_bot
SetEnvIf User-Agent "CCBot" bad_bot
Deny from env=bad_bot
```

It gets the job done, but you're still consuming more resources than with nginx. Just something to keep in mind.

## TIP 3: Middlewareâ€”When You Need Custom Logic

Now, if you want fine-grained control and you're willing to use some application resources, middleware is your tool. This is where you can log which crawlers tried to access what, implement complex rules, or even return custom responses.

Here's a quick Express.js example:
```javascript
app.use((req, res, next) => {
    const userAgent = req.get('user-agent') || '';
    const blockedBots = ['GPTBot', 'anthropic-ai', 'CCBot'];

    if (blockedBots.some(bot => userAgent.includes(bot))) {
        return res.status(429).json({
            error: 'Rate limit exceeded',
            message: 'AI crawler requests are rate limited. Please slow down your requests.',
            retryAfter: 60
        });
    }
    next();
});
```

It's elegant, it's flexible, but rememberâ€”this runs for every request. Use it when you need the extra control.

## TIP 4: robots.txt is Just a Polite Suggestion

Here's something a lot of people don't realize: robots.txt is basically a suggestion. Well-behaved crawlers follow it, but not all AI crawlers do.

```
# Allow specific paths for GPTBot while setting crawl limits
User-agent: GPTBot
Allow: /
Crawl-delay: 10
Request-rate: 5/1m

# Completely block Anthropic's crawler
User-agent: anthropic-ai
Disallow: /

# Default rules for all other crawlers
User-agent: *
Allow: /
Crawl-delay: 5
Request-rate: 10/1m
```

This tells GPTBot and anthropic-ai to stay away, while allowing other crawlers with a 5-second delay. But here's the catchâ€”some aggressive crawlers ignore it completely. So use robots.txt as your first line of defense, but don't rely on it alone.

## TIP 5: Combine Your Defenses

The real power comes from layering your defenses. Use robots.txt for the nice bots, nginx for the aggressive ones, and middleware for custom logic.

Think of it like this:
- **robots.txt** catches the polite crawlers
- **nginx** blocks known AI crawlers at the server level
- **Middleware** gives you application-level control when you need it

Most crawlers will respect robots.txt. The ones that don't get caught by nginx before they waste your resources. And if you need something special, middleware is there.

## TIP 6: Monitor Your Access Logs

Before you start blocking, actually see who's crawling you. Check your server logs for user agents like:
- `GPTBot` (OpenAI)
- `anthropic-ai` (Anthropic)
- `CCBot` (Common Crawl)
- `facebookexternalhit` (Meta)

You can grep your logs like this:

```bash
grep -i "GPTBot\|anthropic-ai" /var/log/nginx/access.log | wc -l
```

This tells you how many times these crawlers hit your site. Knowing is half the battle.

## Recommended Reading

If you want to dive deeper into this stuff:
- [nginx Rate Limiting](https://blog.nginx.org/blog/rate-limiting-nginx)
- [AI Search Crawlers & Bots](https://momenticmarketing.com/blog/ai-search-crawlers-bots)7]

## Thank You for Reading

If you found this helpful, share it with your dev friends and colleagues. Protecting your server resources from unwanted crawlers is just as important as optimizing your code. And if you have any questions or tips of your own, feel free to drop a comment or reach out.

riiiaddesign@gmail.com

Happy blocking! ðŸš€


##### License: MIT

<!--

Contributor's Certificate of Origin

By making a contribution to this project, I certify that:

(a) The contribution was created in whole or in part by me and I have
    the right to submit it under the license indicated in the file; or

(b) The contribution is based upon previous work that, to the best of my
    knowledge, is covered under an appropriate license and I have the
    right under that license to submit that work with modifications,
    whether created in whole or in part by me, under the same license
    (unless I am permitted to submit under a different license), as
    indicated in the file; or

(c) The contribution was provided directly to me by some other person
    who certified (a), (b) or (c) and I have not modified it.

(d) I understand and agree that this project and the contribution are
    public and that a record of the contribution (including all personal
    information I submit with it, including my sign-off) is maintained
    indefinitely and may be redistributed consistent with this project
    or the license(s) involved.

Signed-off-by: riad riiiaddesign@gmail.com

-->