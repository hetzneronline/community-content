---
SPDX-License-Identifier: MIT
path: "/tutorials/tutorial-template"
slug: "building-a-flask-api-to-transcribe-audio-files-using-whisper-ai"
date: "2025-02-03"
title: "Building a Flask API to Transcribe Audio Files using Whisper AI"
short_description: "This tutorial will guide you through building a Flask API to transcribe audio files using Whisper AI. We'll cover setting up the environment, creating the Flask app, and integrating Whisper AI for transcription."
tags: ["Development", "Lang:python"]
author: "Daniel Charles Mwangila"
author_link: "https://github.com/truestbyheart"
author_img: "https://avatars3.githubusercontent.com/u/37019531?v=4"
author_description: "Daniel Charles Mwangila is a software engineer with a passion for building scalable and efficient systems. He has experience working with various technologies and frameworks, and is always looking for new challenges to tackle."
language: "en"
available_languages: ["en"]
header_img: "header-4"
cta: "gpu"
---

## Introduction

## Introduction

In this tutorial, we’ll walk through building a Flask API that leverages OpenAI’s **Whisper AI** to transcribe audio files. The API will handle multiple files concurrently and provide real-time progress updates for each transcription task. By the end of this tutorial, you’ll have a fully functional Flask-based service capable of processing audio files efficiently while keeping users informed about the progress.

**What is Whisper AI?**

Whisper AI is an open-source automatic speech recognition (ASR) model developed by OpenAI. It is designed to convert spoken language into written text with remarkable accuracy across multiple languages. Whisper supports a wide range of tasks, including speech-to-text transcription, language detection, and translation. Its versatility makes it suitable for applications such as transcription services, voice assistants, meeting summaries, podcast indexing, and more.

One of the standout features of Whisper is its ability to handle diverse audio inputs, from clear studio recordings to noisy real-world environments. This robustness stems from its training on a vast dataset of multilingual and multitask audio data, enabling it to generalize well across different accents, dialects, and recording conditions.

**Whisper Models and Their Uses**

Whisper comes in several pre-trained model sizes, each offering a trade-off between performance and resource requirements:

- **Tiny**: The smallest and fastest model, ideal for resource-constrained environments or quick prototyping. It has fewer parameters and is less accurate but highly efficient.
- **Base**: A balanced model suitable for general-purpose transcription tasks. It offers a good mix of speed and accuracy.
- **Small**: Slightly larger and more accurate than the Base model, making it a solid choice for most use cases.
- **Medium**: A high-performance model that delivers excellent accuracy at the cost of increased computational demands.
- **Large**: The most accurate model, designed for scenarios where precision is critical. However, it requires significant computational resources and is best suited for powerful hardware like GPUs.

Each model variant can be selected based on your specific needs. For example, if you’re building a lightweight transcription service for mobile devices, the Tiny model might suffice. On the other hand, if you’re working on a professional-grade transcription platform, the Large model would be more appropriate.

**Benefits of Using Whisper AI**

1. **Multilingual Support**: Whisper supports over 90 languages, making it a versatile tool for global applications.
2. **Open Source**: Being open source, Whisper allows developers to customize and extend its capabilities to fit their unique requirements.
3. **High Accuracy**: Its state-of-the-art architecture ensures reliable transcriptions even in challenging audio conditions.
4. **Ease of Use**: With libraries available in Python, integrating Whisper into your projects is straightforward.

In this tutorial, we’ll focus on using the Base model for transcription, but you can easily switch to other models depending on your needs. We’ll also explore how to build a scalable Flask API that handles multiple transcription tasks concurrently while providing real-time progress updates.

**Prerequisites**
To follow along, you’ll need:

- Basic knowledge of Python and Flask.
- Python 3.8 or higher installed on your machine.
- A working installation of `ffmpeg` (required by Whisper for audio processing).
- An understanding of REST APIs and asynchronous programming in Python (optional but helpful).

If you’re unfamiliar with Flask, consider reading [this introductory tutorial](https://flask.palletsprojects.com/en/latest/quickstart/) before proceeding

---

## Step 1 - Setting Up the Environment

Before diving into the code, let’s set up the project structure and install the necessary dependencies.

### Project Structure

Create the following directory structure:

```
flask-whisper-api/
├── app.py                # Main Flask application
├── requirements.txt      # Python dependencies
├── uploads/              # Folder to store uploaded audio files
└── transcriptions/       # Folder to store transcription results
```

### Installing Dependencies

Create a `requirements.txt` file with the following content:

```
Flask==2.3.2
openai-whisper==20230918
concurrent-log-handler==0.9.20
```

Install the dependencies:

```bash
pip install -r requirements.txt
```

Ensure `ffmpeg` is installed on your system. On Linux, you can install it using:

```bash
sudo apt install ffmpeg
```

On macOS:

```bash
brew install ffmpeg
```

---

## Step 2 - Building the Flask API

Now, let’s create the core Flask application (`app.py`). This script will expose endpoints for uploading audio files, starting transcriptions, and checking progress.

### Step 2.1 - Writing the Flask Application

Here’s the complete code for `app.py`:

```python
from flask import Flask, request, jsonify
import os
import threading
from queue import Queue
from whisper import load_model
from concurrent.futures import ThreadPoolExecutor
import time

app = Flask(__name__)

# Configuration
UPLOAD_FOLDER = 'uploads'
TRANSCRIPTIONS_FOLDER = 'transcriptions'
MAX_WORKERS = 5  # Number of concurrent transcriptions

os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(TRANSCRIPTIONS_FOLDER, exist_ok=True)

# Global state for tracking progress
progress_queue = Queue()
transcription_status = {}

# Load Whisper model
model = load_model("base")  # You can choose other models like "tiny", "small", etc.

@app.route('/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({"error": "No file part"}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400
    
    file_path = os.path.join(UPLOAD_FOLDER, file.filename)
    file.save(file_path)
    
    transcription_id = str(hash(file_path))  # Unique ID for the transcription task
    transcription_status[transcription_id] = {"status": "pending", "progress": 0}
    
    # Add task to the queue
    progress_queue.put((transcription_id, file_path))
    
    return jsonify({
        "message": "File uploaded successfully",
        "transcription_id": transcription_id
    }), 200

@app.route('/progress/<transcription_id>', methods=['GET'])
def get_progress(transcription_id):
    status = transcription_status.get(transcription_id)
    if not status:
        return jsonify({"error": "Invalid transcription ID"}), 404
    
    return jsonify(status), 200

def transcribe_audio(transcription_id, file_path):
    try:
        transcription_status[transcription_id]["status"] = "in_progress"
        
        # Simulate progress updates
        for i in range(1, 11):
            time.sleep(1)  # Simulating transcription work
            transcription_status[transcription_id]["progress"] = i * 10
        
        # Perform actual transcription
        result = model.transcribe(file_path)
        transcription_text = result["text"]
        
        # Save transcription to file
        output_path = os.path.join(TRANSCRIPTIONS_FOLDER, f"{transcription_id}.txt")
        with open(output_path, 'w') as f:
            f.write(transcription_text)
        
        transcription_status[transcription_id]["status"] = "completed"
        transcription_status[transcription_id]["output_path"] = output_path
    except Exception as e:
        transcription_status[transcription_id]["status"] = "failed"
        transcription_status[transcription_id]["error"] = str(e)

def worker():
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        while True:
            transcription_id, file_path = progress_queue.get()
            executor.submit(transcribe_audio, transcription_id, file_path)
            progress_queue.task_done()

# Start background worker thread
threading.Thread(target=worker, daemon=True).start()

if __name__ == '__main__':
    app.run(debug=True)
```

### Step 2.2 - Understanding the Code

The Flask app includes:

1. **Endpoints**:
   - `/upload`: Accepts audio files and queues them for transcription.
   - `/progress/<transcription_id>`: Provides real-time progress updates for a specific transcription task.
2. **Concurrency**: Uses `ThreadPoolExecutor` to handle multiple transcription tasks concurrently.
3. **Progress Tracking**: Tracks the status and progress of each transcription task using a global dictionary (`transcription_status`).

---

## Step 3 - Testing the API

Let’s test the API to ensure everything works as expected.

### Step 3.1 - Starting the Flask Server

Run the Flask server:

```bash
python app.py
```

### Step 3.2 - Uploading an Audio File

Use `curl` or Postman to upload an audio file:

```bash
curl -X POST http://127.0.0.1:5000/upload -F "file=@path/to/audio.mp3"
```

You should receive a response like this:

```json
{
  "message": "File uploaded successfully",
  "transcription_id": "1234567890"
}
```

### Step 3.3 - Checking Progress

Check the progress of the transcription:

```bash
curl http://127.0.0.1:5000/progress/1234567890
```

The response will look like this:

```json
{
  "status": "in_progress",
  "progress": 50
}
```

Once the transcription is complete, the status will change to `"completed"`, and the output file path will be included.

---

## Conclusion

In this tutorial, we built a Flask API that uses Whisper AI to transcribe audio files concurrently. We also implemented a mechanism to provide real-time progress updates for each transcription task. This setup is scalable and can be extended further to include features like authentication, logging, and error handling.

Feel free to experiment with different Whisper models and adjust the concurrency settings based on your hardware capabilities. Happy coding! 🚀

##### License: MIT
<!--
Contributor's Certificate of Origin
By making a contribution to this project, I certify that:
(a) The contribution was created in whole or in part by me and I have
    the right to submit it under the license indicated in the file; or
(b) The contribution is based upon previous work that, to the best of my
    knowledge, is covered under an appropriate license and I have the
    right under that license to submit that work with modifications,
    whether created in whole or in part by me, under the same license
    (unless I am permitted to submit under a different license), as
    indicated in the file; or
(c) The contribution was provided directly to me by some other person
    who certified (a), (b) or (c) and I have not modified it.
(d) I understand and agree that this project and the contribution are
    public and that a record of the contribution (including all personal
    information I submit with it, including my sign-off) is maintained
    indefinitely and may be redistributed consistent with this project
    or the license(s) involved.
Signed-off-by: Daniel Charles Mwangila <daniel@example.com>
-->