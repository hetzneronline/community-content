---
SPDX-License-Identifier: MIT
path: "/tutorials/replacing-faulty-raid-drive"
slug: "replacing-faulty-raid-drive"
date: "2024-12-15"
title: "Replacing a Faulty RAID Drive"
short_description: "Learn how to replace a faulty RAID drive, rebuild the array, and reinstall the bootloader to restore redundancy."
tags: ["RAID", "Linux", "Server Management"]
author: "Stephen Ndegwa"
author_link: "https://github.com/stephenndegwa"
author_img: "https://avatars.githubusercontent.com/u/105418748"
author_description: "System administrator with expertise in Linux and high-availability RAID configurations."
language: "en"
available_languages: ["en"]
header_img: "header-raid"
cta: "product"
---

# Introduction

In this tutorial, you will learn how to replace a faulty RAID drive, rebuild the RAID array, and reinstall the bootloader to ensure proper functionality. By following these steps, you’ll maintain redundancy and restore your RAID array to a healthy state.

This guide is ideal for users familiar with basic Linux administration and RAID setups. It assumes you can access your server through rescue mode.

---

# Prerequisites

Before starting, ensure the following:
- **Access to Server**: Required to replace the faulty drive.
- **Rescue Mode**: Boot your server into rescue mode.
- **Basic RAID Knowledge**: Understanding of RAID concepts (e.g., RAID 1, RAID 5).
- **Tools Installed**:
  - `sgdisk` (for GPT partition management)
  - `mdadm` (for managing the RAID array)
- **Backup**: Create a backup of all critical data before proceeding.

Example terminology used in this tutorial:
- Hostname: `<your_host>`
- New drive: `/dev/sdb`
- Existing intact drive: `/dev/sda`

---

# Step 1 - Verify the New Drive and RAID Status

This step ensures the newly installed drive is recognized, healthy, and ready for integration into the RAID array. It also confirms the current state of the RAID configuration and identifies the degraded arrays.

### List All Drives

To verify the new drive and existing disks, use the `lsblk` command:

```bash
lsblk
```

**Expected Output:**
```plaintext
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINTS
loop0     7:0    0   3.2G  1 loop  
sda       8:0    0 465.8G  0 disk  
├─sda1    8:1    0    16G  0 part  
│ └─md0   9:0    0    16G  0 raid1 
├─sda2    8:2    0     1G  0 part  
│ └─md1   9:1    0  1022M  0 raid1 
└─sda3    8:3    0 448.8G  0 part  
  └─md2   9:2    0 448.6G  0 raid1 
sdb       8:16   0 465.8G  0 disk  
```

- **Explanation:**
  - `sda`: The existing healthy drive containing RAID partitions (`md0`, `md1`, `md2`).
  - `sdb`: The new drive, detected but unpartitioned.

If `sdb` does not appear in the list, check physical connections or reboot into rescue mode again.

### Check RAID Status

Verify the current RAID configuration using:

```bash
cat /proc/mdstat
```

**Expected Output:**
```plaintext
Personalities : [raid1] 
md2 : active raid1 sda3[1]
      470426624 blocks super 1.2 [2/1] [_U]
      bitmap: 4/4 pages [16KB], 65536KB chunk

md1 : active raid1 sda2[1]
      1046528 blocks super 1.2 [2/1] [_U]
      
md0 : active raid1 sda1[1]
      16759808 blocks super 1.2 [2/1] [_U]
      
unused devices: <none>
```

- **Explanation:**
  - `md2`, `md1`, and `md0` are RAID arrays in degraded mode (`[2/1] [_U]`).
  - `_U` indicates the array is missing one drive, but the remaining drive (`sda`) is operational.

If no RAID arrays are listed, or the output differs, check for misconfiguration using `mdadm --detail` commands.

### Verify New Drive Health

Check the health status of the new drive (`/dev/sdb`) using SMART (Self-Monitoring, Analysis, and Reporting Technology):

```bash
smartctl -a /dev/sdb
```

**Expected Output (Key Sections):**
```plaintext
Device Model:     Samsung SSD 850 EVO 500GB
SMART overall-health self-assessment test result: PASSED
Reallocated_Sector_Ct: 0
Power_On_Hours: 66955
Power_Cycle_Count: 25
```

- **Explanation:**
  - **SMART overall-health self-assessment test result:** Should display `PASSED`.
  - **Reallocated_Sector_Ct:** Indicates bad sectors replaced with spares. Should be `0`.
  - **Power_On_Hours:** Reflects the total usage time. This is fine for a used drive.
  - **Error Log:** No errors logged.

If the result is anything other than `PASSED`, the drive may not be suitable for RAID. Replace the drive.

### Clear the New Drive

Ensure the new drive is in a clean state by wiping its first sectors:

```bash
dd if=/dev/zero of=/dev/sdb bs=1M count=10
```

**Expected Output:**
```plaintext
10+0 records in
10+0 records out
10485760 bytes (10 MB, 10 MiB) copied, 0.0532937 s, 197 MB/s
```

- **Explanation:**
  - This command overwrites the first 10 MB of the drive with zeros, removing any old data, metadata, or signatures.
  - This prevents conflicts when adding the drive to the RAID array.

---

# Step 2 - Partition Table Setup for the New Drive

In this step, we copy the partition table from the existing healthy drive (`/dev/sda`) to the new drive (`/dev/sdb`). We also ensure the new drive is uniquely identified with a new UUID to avoid conflicts.

### Backup the Partition Table from the Active Drive

The first step is to back up the partition table from the existing drive (`/dev/sda`). This ensures we replicate the exact partition structure on the new drive.

Run the following command:

```bash
sgdisk --backup=/tmp/sda-partition-table.gpt /dev/sda
```

**Expected Output:**
```plaintext
***************************************************************
Found invalid GPT and valid MBR; converting MBR to GPT format
in memory. 
***************************************************************
The operation has completed successfully.
```

**Explanation:**
- The command creates a backup of the partition table from `/dev/sda` and saves it as `/tmp/sda-partition-table.gpt`.
- If you encounter errors, ensure the drive (`/dev/sda`) is healthy and recognized.

**Verification:**
Check that the backup file exists:
```bash
ls -l /tmp/sda-partition-table.gpt
```

**Expected Output:**
```plaintext
-rw-r--r-- 1 root root 18K Dec 15 06:14 /tmp/sda-partition-table.gpt
```

### Restore the Partition Table to the New Drive

Now, restore the saved partition table to the new drive (`/dev/sdb`) using:

```bash
sgdisk --load-backup=/tmp/sda-partition-table.gpt /dev/sdb
```

**Expected Output:**
```plaintext
Creating new GPT entries in memory.
The operation has completed successfully.
```

**Explanation:**
- The partition structure from `/dev/sda` is now applied to `/dev/sdb`.

### Assign a New UUID to the New Drive

To ensure the new drive has a unique identifier, assign a new UUID (Globally Unique Identifier):

```bash
sgdisk -G /dev/sdb
```

**Expected Output:**
```plaintext
The operation has completed successfully.
```

**Explanation:**
- Each drive in the RAID setup must have a unique identifier to prevent conflicts.
- The UUID can be verified in the output of the next step.

### Verify the Partition Structure

Verify that the partition table on `/dev/sdb` matches `/dev/sda`:

```bash
fdisk -l /dev/sdb
```

**Expected Output:**
```plaintext
Disk /dev/sdb: 465.76 GiB, 500107862016 bytes, 976773168 sectors
Disk model: Samsung SSD 850 
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: gpt
Disk identifier: 033F9E88-57F6-4CF4-869E-673B6C3770DF

Device        Start       End   Sectors   Size Type
/dev/sdb1      2048  33556479  33554432    16G Linux RAID
/dev/sdb2  33556480  35653631   2097152     1G Linux RAID
/dev/sdb3  35653632 976771119 941117488 448.8G Linux RAID
```

**Explanation:**
- The partition structure on `/dev/sdb` now matches `/dev/sda`.
- The disk identifier (`Disk identifier: 033F9E88-57F6-4CF4-869E-673B6C3770DF`) confirms the new UUID.

---

# Step 3 - Add the New Drive to the RAID Array

In this step, we add the partitions from the new drive (`/dev/sdb`) to the degraded RAID arrays (`md0`, `md1`, `md2`) and start the synchronization process.

### Add Partitions to the RAID Arrays

Use the `mdadm` command to add each partition from the new drive (`/dev/sdb`) to the corresponding RAID array.

Run the following commands:

```bash
mdadm /dev/md0 --add /dev/sdb1
mdadm /dev/md1 --add /dev/sdb2
mdadm /dev/md2 --add /dev/sdb3
```

**Expected Output (for each command):**
```plaintext
mdadm: added /dev/sdb1
mdadm: added /dev/sdb2
mdadm: added /dev/sdb3
```

- **Explanation:**
  - Each command adds the corresponding partition from `/dev/sdb` to the respective RAID array.
  - Ensure the RAID array and partition numbers match (`md0` with `/dev/sdb1`, etc.).

### Verify RAID Status

After adding the partitions, monitor the RAID synchronization process using:

```bash
cat /proc/mdstat
```

**Example Output (during sync):**
```plaintext
Personalities : [raid1] 
md2 : active raid1 sda3[1] sdb3[2]
      470426624 blocks super 1.2 [2/2] [UU]
      [>....................]  resync = 10.3% (4857600/470426624) finish=5.4min speed=148720K/sec

md1 : active raid1 sda2[1] sdb2[2]
      1046528 blocks super 1.2 [2/2] [UU]
      [>....................]  resync = 25.0% (261632/1046528) finish=1.2min speed=872K/sec
     
md0 : active raid1 sda1[1] sdb1[2]
      16759808 blocks super 1.2 [2/2] [UU]
      [===>.................]  resync = 50.0% (8389904/16759808) finish=2.4min speed=567K/sec
```

**Key Observations:**
- `[>...........]`: Indicates the progress of the resynchronization.
- `[UU]`: Indicates both drives are active in the RAID array (healthy state).

**Important:**
- Synchronization speed depends on the size of the array and system performance.
- The `finish` estimate shows the time remaining for the sync.

### Check Detailed RAID Information

To view detailed RAID status and ensure the new partitions are being synchronized properly:

```bash
mdadm --detail /dev/md0
mdadm --detail /dev/md1
mdadm --detail /dev/md2
```

**Example Output:**
```plaintext
/dev/md0:
           Version : 1.2
     Creation Time : Mon Dec 15 06:30:00 2024
        Raid Level : raid1
        Array Size : 16759808 (16.0 GiB 17.2 GB)
     Used Dev Size : 16759808 (16.0 GiB 17.2 GB)
      Raid Devices : 2
     Total Devices : 2
       Persistence : Superblock is persistent

     Update Time : Mon Dec 15 06:45:00 2024
           State : clean, resyncing 
  Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : rescue:0  (local to host rescue)
           UUID : e832a6d0:5c47b5ed:4b2fbd10:84391e0e
         Events : 32

    Number   Major   Minor   RaidDevice State
       0       8        1        0      active sync   /dev/sda1
       1       8       17        1      active sync   /dev/sdb1
```

- **Explanation:**
  - The `State` should eventually show `clean` and `active sync` after synchronization completes.
  - Both devices (`/dev/sdaX` and `/dev/sdbX`) should appear as `active sync`.

### Wait for Synchronization to Complete

Use the `watch` command to monitor synchronization in real time:

```bash
watch cat /proc/mdstat
```

- **Output (real-time monitoring):**
  The output refreshes every 2 seconds, showing sync progress.

### Save the RAID Configuration

Once the synchronization process is complete, update the RAID configuration file to include the new drive:

```bash
mdadm --detail --scan >> /etc/mdadm/mdadm.conf
```

**Explanation:**
- This command appends the current RAID array configuration to the configuration file, ensuring it persists across reboots.

---

# Step 4 - Reinstall the Bootloader

1. **Mount the RAID Arrays**:
   ```bash
   mount /dev/md2 /mnt
   mount /dev/md1 /mnt/boot
   ```

2. **Bind Essential Directories**:
   ```bash
   mount --bind /dev /mnt/dev
   mount --bind /proc /mnt/proc
   mount --bind /sys /mnt/sys
   ```

3. **Chroot into the System**:
   ```bash
   chroot /mnt
   ```

4. **Reinstall GRUB on Both Drives**:
   ```bash
   grub-install /dev/sda
   grub-install /dev/sdb
   update-grub
   ```

5. **Exit Chroot**:
   ```bash
   exit
   ```

6. **Unmount Directories**:
   ```bash
   umount /mnt/dev /mnt/proc /mnt/sys /mnt/boot /mnt
   ```

---

# Conclusion

Congratulations! You’ve successfully replaced a faulty RAID drive. You’ve synchronized the RAID arrays and reinstalled the bootloader to ensure the server can boot from either drive. To maintain RAID health, periodically check the RAID status using:
```bash
cat /proc/mdstat
mdadm --detail /dev/mdX
```

If you encounter any issues, support and the community are always available to assist.

### License: MIT

<!--

Contributor's Certificate of Origin

By making a contribution to this project, I certify that:

(a) The contribution was created in whole or in part by me and I have
    the right to submit it under the license indicated in the file; or

(b) The contribution is based upon previous work that, to the best of my
    knowledge, is covered under an appropriate license and I have the
    right under that license to submit that work with modifications,
    whether created in whole or in part by me, under the same license
    (unless I am permitted to submit under a different license), as
    indicated in the file; or

(c) The contribution was provided directly to me by some other person
    who certified (a), (b) or (c) and I have not modified it.

(d) I understand and agree that this project and the contribution are
    public and that a record of the contribution (including all personal
    information I submit with it, including my sign-off) is maintained
    indefinitely and may be redistributed consistent with this project
    or the license(s) involved.

Signed-off-by: Stephen Ndegwa - stephenndegwairungu@gmail.com

-->

