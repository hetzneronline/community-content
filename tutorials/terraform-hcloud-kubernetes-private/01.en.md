# Terraform Hetzner Cloud Talos Kubernetes Cluster Tutorial

This tutorial guides you through setting up a private, production-ready Kubernetes cluster using Talos OS on Hetzner Cloud with proper NAT gateway configuration.

## Architecture Overview

```
                    Internet
                        │
                        ▼
    ┌─────────────────────────────────────────┐
    │     Hetzner Cloud Load Balancer         │
    │        (Managed by Kubernetes)          │
    │           (Public IP)                   │
    │  ┌─────────────┐  ┌─────────────────┐   │
    │  │   Port 80   │  │   Port 443      │   │
    │  │  → :30000   │  │  → :30001       │   │
    │  └─────────────┘  └─────────────────┘   │
    └─────────────────────────────────────────┘
                        │
                        ▼
    ┌─────────────────────────────────────────┐
    │        Private Network                  │
    │         (10.21.0.0/16)                  │
    │                                         │
    │  ╔═════════════════════════════════╗    │
    │  ║                                 ║    │
    │  ║     🎯 KUBERNETES CLUSTER       ║    │
    │  ║                                 ║    │
    │  ║  ┌─────────────────────────┐    ║    │
    │  ║  │   Control Plane Nodes   │    ║    │
    │  ║  │      (3 Talos VMs)      │    ║    │
    │  ║  │   - etcd cluster        │    ║    │
    │  ║  │   - kube-apiserver      │    ║    │
    │  ║  │   - kube-scheduler      │    ║    │
    │  ║  └─────────────────────────┘    ║    │
    │  ║                                 ║    │
    │  ║  ┌─────────────────────────┐    ║    │
    │  ║  │    Worker Nodes         │    ║    │
    │  ║  │    (Talos VMs)          │    ║    │
    │  ║  │   - kubelet             │    ║    │
    │  ║  │   - kube-proxy          │    ║    │
    │  ║  │   - ingress-nginx       │    ║    │
    │  ║  │   - Longhorn storage    │    ║    │
    │  ║  │   - NodePorts 30000/1   │    ║    │
    │  ║  └─────────────────────────┘    ║    │
    │  ╚═════════════════════════════════╝    │
    │                                         │
    │  ┌─────────────────────────────────┐    │
    │  │        🌐 Egress VM             │    │
    │  │      (Management Node)          │    │
    │  │   - Public IP (SSH access)      │    │
    │  │   - Private IP (NAT gateway)    │    │
    │  │   - kubectl/talosctl tools      │    │
    │  │   - Routes traffic to internet  │    │
    │  └─────────────────────────────────┘    │
    └─────────────────────────────────────────┘
```

**Traffic Flow:**
- **Inbound**: Internet → Load Balancer → Kubernetes Worker NodePorts → ingress-nginx → Services
- **Outbound**: Kubernetes Nodes → Egress VM (NAT) → Internet  
- **Management**: SSH to Egress VM → kubectl/talosctl to Kubernetes cluster

## Prerequisites

- Hetzner Cloud account and API token
- Terraform installed (>= 1.0)
- Basic knowledge of Kubernetes and networking
- Understanding of Talos OS concepts

## Step 1: Set Up NAT Gateway and Private Network

Before creating the Kubernetes cluster, you need to set up a private network with NAT gateway for internet access. Follow the official Hetzner tutorial:

**[How to set up NAT for Cloud Networks](https://community.hetzner.com/tutorials/how-to-set-up-nat-for-cloud-networks)**

This tutorial covers:
- Creating a private network
- Setting up a NAT gateway VM
- Configuring routing and firewall rules
- Getting your network ID (needed for `hcloud_network_id` below)

## Step 2: Create Project Directory

```bash
mkdir k8s-cluster && cd k8s-cluster
```

## Step 3: Configure Terraform Variables

Create `variables.tf`:

```hcl
variable "hcloud_token" {
  description = "Hetzner Cloud API token. Prefer to supply via TF_VAR_hcloud_token or terraform.tfvars, or rely on provider using HCLOUD_TOKEN env."
  type        = string
  sensitive   = true
}
```

<details>
<summary><strong>Optional: S3 Backup Variables (Click to expand)</strong></summary>

If you want to enable S3 backup for Talos configuration, add these additional variables to your `variables.tf`:

```hcl
variable "talos_backup_s3_access_key" {
  description = "S3 Access Key for Talos Backup."
  type        = string
  sensitive   = true
  default     = ""
}

variable "talos_backup_s3_secret_key" {
  description = "S3 Secret Access Key for Talos Backup."
  type        = string
  sensitive   = true
  default     = ""
}

variable "talos_backup_s3_bucket" {
  description = "S3 bucket name for Talos backups."
  type        = string
  default     = ""
}

variable "talos_backup_s3_endpoint" {
  description = "S3 endpoint hostname for Talos backups."
  type        = string
  default     = ""
}

variable "talos_backup_s3_region" {
  description = "S3 region for Talos backups."
  type        = string
  default     = ""
}
```

</details>

### Token Configuration

Set your token via environment variable or terraform.tfvars:

```bash
# Option 1: Environment variable
export TF_VAR_hcloud_token="your-hetzner-cloud-api-token"

# Option 2: Create terraform.tfvars file
echo 'hcloud_token = "your-hetzner-cloud-api-token"' > terraform.tfvars
```

Create `kubernetes.tf` with the main cluster configuration:

```hcl
module "kubernetes" {
  source  = "hcloud-k8s/kubernetes/hcloud"
  version = "2.7.3"

  cluster_name = "k8s"
  hcloud_token = var.hcloud_token

  # Export configs for Talos and Kube API access
  cluster_kubeconfig_path  = "kubeconfig"
  cluster_talosconfig_path = "talosconfig"

  # Optional Ingress Controller, Cert Manager and Storage
  cert_manager_enabled  = true
  ingress_nginx_enabled = true
  longhorn_enabled      = true

  network_ipv4_cidr = "10.21.0.0/16"

  # Private nodes, egress via your own gateway VM
  talos_public_ipv4_enabled = false
  talos_public_ipv6_enabled = false

  control_plane_nodepools = [
    { name = "control", type = "cx22", location = "fsn1", count = 3 }
  ]
  
  worker_nodepools = [
    # placement_group = true ensures VMs are distributed across different physical servers
    { name = "worker-fsn-ccx", type = "ccx23", location = "fsn1", count = 3, placement_group = true },
  ]

  cluster_healthcheck_enabled = true
  firewall_use_current_ipv4 = false
  firewall_use_current_ipv6 = false
  cluster_access = "private"
  talos_extra_routes = ["0.0.0.0/0"]
  network_native_routing_cidr = "10.0.0.0/8"

  # Use your existing Network ID from the NAT gateway setup (Step 1)
  # You can find this in Hetzner Cloud Console -> Networks or via: hcloud network list
  hcloud_network_id = YOUR_NETWORK_ID_HERE

  control_plane_private_vip_ipv4_enabled = true
  ingress_nginx_kind = "DaemonSet"
  ingress_nginx_service_external_traffic_policy = "Local"

  ingress_load_balancer_pools = [
    {
      name          = "regional-lb-fsn"
      location      = "fsn1"
    }
  ]

  cluster_autoscaler_nodepools = [
    {
      name     = "autoscaler"
      type     = "ccx23"
      location = "fsn1"
      min      = 0
      max      = 6
      labels   = { "autoscaler-node" = "true" }
      taints   = [ "autoscaler-node=true:NoExecute" ]
    }
  ]

  cluster_delete_protection = true
}
```

<details>
<summary><strong>Optional: S3 Backup Configuration (Click to expand)</strong></summary>

If you added the S3 backup variables to your `variables.tf`, include these lines in your `kubernetes.tf` module configuration:

```hcl
# Add these lines inside the module "kubernetes" block above
talos_backup_s3_endpoint   = var.talos_backup_s3_endpoint
talos_backup_s3_region     = var.talos_backup_s3_region
talos_backup_s3_bucket     = var.talos_backup_s3_bucket
talos_backup_s3_access_key = var.talos_backup_s3_access_key
talos_backup_s3_secret_key = var.talos_backup_s3_secret_key
```

</details>

## Configuration Files Summary

You'll need these files in your working directory:

1. **variables.tf** - Variable definitions (with optional S3 variables)
2. **kubernetes.tf** - Main cluster configuration (with optional S3 configuration)
3. **terraform.tfvars** - Your actual values (or use environment variables)

## Step 4: Deploy the Cluster

Initialize and apply the Terraform configuration:

```bash
terraform init -upgrade
terraform apply
```

Review the planned changes and confirm the deployment. This process will:
1. Create Talos images using Packer
2. Deploy control plane and worker nodes
3. Configure the Kubernetes cluster
4. Set up ingress controllers and cert-manager
5. Configure Longhorn for persistent storage

## Step 5: Access Your Cluster

After successful deployment, you'll find the configuration files in your current directory:

```bash
export TALOSCONFIG=talosconfig
export KUBECONFIG=kubeconfig
```

Verify your cluster is running:

```bash
# Check Talos cluster members
talosctl get member

# Check Kubernetes nodes
kubectl get nodes -o wide

# Check all pods across namespaces
kubectl get pods -A
```

## Configuration Highlights

### Private Network Setup
- **Network CIDR**: `10.21.0.0/16`
- **No public IPs**: All Kubernetes nodes are private
- **Egress routing**: Traffic flows through your gateway VM

### High Availability Features
- **Control plane**: 3 nodes for HA
- **Worker nodes**: Distributed across placement groups (ensures VMs run on different physical hardware for better reliability)
- **Ingress**: Load balancer with DaemonSet configuration
- **Storage**: Longhorn for distributed persistent storage

### Security Features
- **Private cluster access**: No direct internet access to nodes
- **Firewall**: Controlled access through security groups
- **Backup**: Optional S3 backup for Talos configuration

## Scaling to Full High Availability

For a production-ready, fully high-available setup, consider these enhancements:

### Multiple Egress VMs
Deploy additional egress VMs in different locations with failover configuration:
- Set up multiple gateway VMs across different zones
- Configure VRRP (Virtual Router Redundancy Protocol) for automatic failover
- Use BGP routing for advanced traffic management

### Multi-Region Load Balancers
```hcl
ingress_load_balancer_pools = [
  {
    name          = "regional-lb-fsn"
    location      = "fsn1"
    local_traffic = true
  },
  {
    name          = "regional-lb-nbg"
    location      = "nbg1"
    local_traffic = true
  },
  {
    name          = "regional-lb-hel"
    location      = "hel1"
    local_traffic = true
  }
]
```

### Cross-Zone Worker Distribution
```hcl
worker_nodepools = [
  # Each placement_group = true ensures nodes within each location are on different physical servers
  { name = "worker-fsn", type = "cpx41", location = "fsn1", count = 2, placement_group = true },
  { name = "worker-nbg", type = "cpx41", location = "nbg1", count = 2, placement_group = true },
  { name = "worker-hel", type = "cpx41", location = "hel1", count = 2, placement_group = true },
]
```

### Additional HA Components
- **External DNS**: Automatic DNS management for services
- **Monitoring**: Prometheus and Grafana for observability
- **Backup strategies**: Regular etcd and persistent volume backups
- **Disaster recovery**: Cross-region backup and restore procedures

This setup provides a robust, private Kubernetes cluster that can handle production workloads while maintaining security and high availability standards.

## Troubleshooting

### Common Issues
1. **Network connectivity**: Ensure your egress VM's NAT rules are correctly configured
2. **DNS resolution**: Verify that private nodes can resolve external DNS through the gateway
3. **Load balancer access**: Check that ingress controllers are properly configured for private network access

### Useful Commands
```bash
# Check Talos node status
talosctl -n <node-ip> get nodeready

# Check network connectivity from nodes
talosctl -n <node-ip> get links

# Restart Talos services if needed
talosctl -n <node-ip> restart systemd-networkd
```

With this configuration, you have a fully private, production-ready Kubernetes cluster running on Hetzner Cloud that can scale to meet your needs while maintaining high security and availability standards.
